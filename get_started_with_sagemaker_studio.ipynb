{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Different Combinations of SageMaker Components\n",
    "\n",
    "1. [Example 1: build, train, and deploy _**all**_ on SageMaker](#Example1)\n",
    "\n",
    "2. [Example 2: bring own training script](#Example2)\n",
    "\n",
    "3. [Example 3: bring own trained model](#Example3)\n",
    "\n",
    "4. [Example 4: bring own container](#Example4)\n",
    "\n",
    "Useful Docs:\n",
    "\n",
    "1. [AWS SDK for Python (Boto3)](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "\n",
    "2. [Amazon SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Example1\"></a>\n",
    "# Example 1: build, train, and deploy _**all**_ on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1. Introduction\n",
    "2. Prerequisites and Preprocessing\n",
    "    1. Data ingestion\n",
    "    2. Data inspection\n",
    "    3. Data conversion\n",
    "    4. Data uploading\n",
    "3. Training\n",
    "4. Hosting\n",
    "5. Validating\n",
    "6. (Optional) Clean-up\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Using Amazon SageMaker's Built-in Linear Learner to Predict Whether a Handwritten Digit is a 0.\n",
    "\n",
    "> Amazon SageMaker's Linear Learner algorithm extends upon typical linear models by training many models in parallel, in a computationally efficient manner. Each model has a different set of hyperparameters, and then the algorithm finds the set that optimizes a specific criteria. This can provide substantially more accurate models than typical linear algorithms at the same, or lower, cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Preprocessing\n",
    "\n",
    "Kernel in SageMaker Studio: `Data Science`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker.Session().default_bucket()  # “sagemaker-{region}-{aws-account-id}”\n",
    "prefix = 'demo-linear-mnist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next, we read the dataset into memory, for preprocessing prior to training. This processing could be done *in situ* by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle, gzip\n",
    "\n",
    "# Manually download mnist.pkl.gz from https://www.kaggle.com/pablotab/mnistpklgz\n",
    "# Upload mnist.pkl.gz to ./datasets\n",
    "\n",
    "# Load the dataset\n",
    "with gzip.open('./datasets/mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection\n",
    "\n",
    "Once the dataset is imported, it's typical as part of the machine learning process to inspect the data, understand the distributions, and determine what type(s) of preprocessing might be needed. You can perform those tasks right here in the notebook. As an example, let's go ahead and look at one of the digits that is part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (2, 10)\n",
    "\n",
    "\n",
    "def show_digit(img, caption='', subplot=None):\n",
    "    if subplot == None:\n",
    "        _, (subplot) = plt.subplots(1, 1)\n",
    "    imgr = img.reshape((28, 28))\n",
    "    subplot.axis('off')\n",
    "    subplot.imshow(imgr, cmap='gray')\n",
    "    plt.title(caption)\n",
    "\n",
    "show_digit(train_set[0][30], 'This is a {}'.format(train_set[1][30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data conversion\n",
    "\n",
    "Since algorithms have particular input and output requirements, converting the dataset is also part of the process that a data scientist goes through prior to initiating training. In this particular case, the Amazon SageMaker implementation of Linear Learner takes recordIO-wrapped protobuf, where the data we have today is a pickle-ized numpy array on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "vectors = np.array([t.tolist() for t in train_set[0]]).astype('float32')\n",
    "labels = np.where(np.array([t.tolist() for t in train_set[1]]) == 0, 1, 0).astype('float32')\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, vectors, labels)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data uploading\n",
    "\n",
    "Now that we've created our recordIO-wrapped protobuf, we'll need to upload it to S3, so that Amazon SageMaker training can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "key = 'recordio-pb-data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3url_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print(s3url_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also setup an output S3 location for the model artifact that will be output as the result of training with the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3url_output = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print(s3url_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "First let's specify our algorithm image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "image = get_image_uri(boto3.Session().region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll kick off the base estimator, making sure to pass in the necessary hyperparameters.  Notice:\n",
    "- `feature_dim` is set to 784, which is the number of pixels in each 28 x 28 image.\n",
    "- `predictor_type` is set to 'binary_classifier' since we are trying to predict whether the image is or is not a 0.\n",
    "- `mini_batch_size` is set to 200.  This value can be tuned for relatively minor improvements in fit and speed, but selecting a reasonable value relative to the dataset is appropriate in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "linear = sagemaker.estimator.Estimator(image,\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.m5.xlarge',\n",
    "                                       output_path=s3url_output,\n",
    "                                       sagemaker_session=sagemaker.Session())\n",
    "\n",
    "linear.set_hyperparameters(feature_dim=784,\n",
    "                           predictor_type='binary_classifier',\n",
    "                           mini_batch_size=200)\n",
    "\n",
    "linear.fit({'train': s3url_train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting\n",
    "\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint.  This will allow us to make predictions (or inference) from the model dyanamically. (_Note, Amazon SageMaker allows you the flexibility of importing models trained elsewhere, as well as the choice of not importing models if the target of model creation is AWS Lambda, AWS Greengrass, Amazon Redshift, Amazon Athena, or other deployment target._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Use an existing model trained by SageMaker\n",
    "\n",
    "job_name = 'linear-learner-2021-02-20-17-21-37-544'\n",
    "linear = sagemaker.estimator.Estimator.attach(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instance_types = [\"ml.r5d.12xlarge\", \"ml.r5.12xlarge\", \"ml.p2.xlarge\", \"ml.m5.4xlarge\", \"ml.m4.16xlarge\", \"ml.r5d.24xlarge\", \"ml.r5.24xlarge\", \"ml.p3.16xlarge\", \"ml.m5d.xlarge\", \"ml.m5.large\", \"ml.t2.xlarge\", \"ml.p2.16xlarge\", \"ml.m5d.12xlarge\", \"ml.inf1.2xlarge\", \"ml.m5d.24xlarge\", \"ml.c4.2xlarge\", \"ml.c5.2xlarge\", \"ml.c4.4xlarge\", \"ml.inf1.6xlarge\", \"ml.c5d.2xlarge\", \"ml.c5.4xlarge\", \"ml.g4dn.xlarge\", \"ml.g4dn.12xlarge\", \"ml.c5d.4xlarge\", \"ml.g4dn.2xlarge\", \"ml.c4.8xlarge\", \"ml.c4.large\", \"ml.c5d.xlarge\", \"ml.c5.large\", \"ml.g4dn.4xlarge\", \"ml.c5.9xlarge\", \"ml.g4dn.16xlarge\", \"ml.c5d.large\", \"ml.c5.xlarge\", \"ml.c5d.9xlarge\", \"ml.c4.xlarge\", \"ml.inf1.xlarge\", \"ml.g4dn.8xlarge\", \"ml.inf1.24xlarge\", \"ml.m5d.2xlarge\", \"ml.t2.2xlarge\", \"ml.c5d.18xlarge\", \"ml.m5d.4xlarge\", \"ml.t2.medium\", \"ml.c5.18xlarge\", \"ml.r5d.2xlarge\", \"ml.r5.2xlarge\", \"ml.p3.2xlarge\", \"ml.m5d.large\", \"ml.m5.xlarge\", \"ml.m4.10xlarge\", \"ml.t2.large\", \"ml.r5d.4xlarge\", \"ml.r5.4xlarge\", \"ml.m5.12xlarge\", \"ml.m4.xlarge\", \"ml.m5.24xlarge\", \"ml.m4.2xlarge\", \"ml.p2.8xlarge\", \"ml.m5.2xlarge\", \"ml.r5d.xlarge\", \"ml.r5d.large\", \"ml.r5.xlarge\", \"ml.r5.large\", \"ml.p3.8xlarge\", \"ml.m4.4xlarge\"]\n",
    "for i in range(len(all_instance_types)):\n",
    "    try:\n",
    "        linear_predictor = linear.deploy(initial_instance_count=1,\n",
    "                                         instance_type=all_instance_types[i])\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        print('\\nUsing instance type: ', all_instance_types[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating\n",
    "\n",
    "Finally, we can now validate the model for use.  We can pass HTTP POST requests to the endpoint to get back predictions.  We'll specify how to serialize requests and deserialize responses that are specific to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "# linear_predictor.content_type = 'text/csv'\n",
    "linear_predictor.serializer = csv_serializer\n",
    "linear_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to get a prediction for a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = linear_predictor.predict(train_set[0][30:31])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see that for one record our endpoint returned some JSON which contains `predictions`, including the `score` and `predicted_label`.  In this case, `score` will be a continuous value between [0, 1] representing the probability we think the digit is a 0 or not.  `predicted_label` will take a value of either `0` or `1` where (somewhat counterintuitively) `1` denotes that we predict the image is a 0, while `0` denotes that we are predicting the image is not of a 0.\n",
    "\n",
    "Let's do a whole batch of images and evaluate our predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = []\n",
    "for array in np.array_split(test_set[0], 100):\n",
    "    result = linear_predictor.predict(array)\n",
    "    predictions += [r['predicted_label'] for r in result['predictions']]\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.crosstab(np.where(test_set[1] == 0, 1, 0), predictions, rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Clean-up\n",
    "\n",
    "To avoid incurring unnecessary charges, delete the endpoints and resources that you created while running this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(linear_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pprint import pprint\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "\n",
    "def main():\n",
    "    model_names = []\n",
    "    for key in paginate(client.list_models):\n",
    "        model_names.append(key['ModelName'])\n",
    "    delete_multiple_models(model_names)\n",
    "\n",
    "\n",
    "def delete_multiple_models(model_names):\n",
    "    for model_name in model_names:\n",
    "        print('Deleting model: {}'.format(model_name))\n",
    "        client.delete_model(ModelName=model_name)\n",
    "\n",
    "\n",
    "def paginate(method, **kwargs):\n",
    "    client = method.__self__\n",
    "    paginator = client.get_paginator(method.__name__)\n",
    "    for page in paginator.paginate(**kwargs).result_key_iters():\n",
    "        for result in page:\n",
    "            yield result\n",
    "\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete S3 bucket\n",
    "\n",
    "Open the Amazon S3 [console](https://console.aws.amazon.com/s3/), and then delete the bucket that you created for storing model artifacts and the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete logs\n",
    "\n",
    "Open the Amazon CloudWatch [console](https://console.aws.amazon.com/cloudwatch/), and then delete all of the log groups that have names starting with `/aws/sagemaker/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Example2\"></a>\n",
    "# Example 2: bring own training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1. Introduction\n",
    "2. Prerequisites and Preprocessing\n",
    "    1. Data ingestion\n",
    "    2. Data inspection\n",
    "    3. Data uploading\n",
    "3. Debugger\n",
    "4. Experiments\n",
    "5. Monitor\n",
    "6. (Optional) Clean-up\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Using Gradient Boosted Trees as a Framework to Predict Mobile Customer Departure.\n",
    "\n",
    "> Using XGBoost **as a framework** provides more flexibility than using it **as a built-in algorithm** because it enables more advanced scenarios that allow you to incorporate preprocessing and post-processing scripts into your training script. Specifically, we'll be able to specify a list of rules that we want Debugger to evaluate our training process against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Preprocessing\n",
    "\n",
    "Kernel in SageMaker Studio: `Data Science`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the Sagemaker Python SDK Version\n",
    "# If you see the \"Please restart the kernel\" prompt, simply click Kernel above and hit Restart.\n",
    "import sagemaker\n",
    "\n",
    "if int(sagemaker.__version__.split('.')[0]) == 2:\n",
    "    !{sys.executable} -m pip install \"sagemaker>=1.71.0,<2.0.0\"\n",
    "    print(\"Installing previous SageMaker Version. Please restart the kernel\")\n",
    "else:\n",
    "    print(\"Version is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from IPython.display import display\n",
    "from time import strftime, gmtime\n",
    "import awscli\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig\n",
    "from sagemaker.model_monitor import DataCaptureConfig, DatasetFormat, DefaultModelMonitor\n",
    "from sagemaker.s3 import S3Uploader, S3Downloader\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "# sess.get_available_services()\n",
    "sm = sess.client('sagemaker')\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker.Session().default_bucket()  # “sagemaker-{region}-{aws-account-id}”\n",
    "prefix = 'demo-xgboost-churn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "The dataset that we use is publicly available and was mentioned in the book [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) by Daniel T. Larose. It's attributed by the author to the University of California Irvine Repository of Machine Learning Datasets. The downloaded and preprocessed dataset is in the `data` folder that accompanies the [notebook](https://github.com/aws/amazon-sagemaker-examples/blob/master/aws_sagemaker_studio/getting_started/xgboost_customer_churn_studio.ipynb). It's been split into training and validation datasets. To see how the dataset was preprocessed, see this [XGBoost customer churn notebook that starts with the original dataset](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the data files\n",
    "# Make sure you cloned https://github.com/aws/amazon-sagemaker-examples.git\n",
    "%cd /root/amazon-sagemaker-examples/aws_sagemaker_studio/getting_started\n",
    "local_data_path = './data/training-dataset-with-header.csv'\n",
    "data = pd.read_csv(local_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection\n",
    "\n",
    "We'll train on a CSV file without the header. But for now, we load some of the data from a version of the training data that has a header. Explore the data to see the dataset's features and what data will be used to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 10)         # Keep the output on one page\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data uploading\n",
    "\n",
    "Now we'll upload the files to S3 for training but first we will create an S3 bucket for the data if one does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if sess.region_name == \"us-east-1\":\n",
    "        sess.client('s3').create_bucket(Bucket=bucket)\n",
    "    else:\n",
    "        sess.client('s3').create_bucket(Bucket=bucket, \n",
    "                                        CreateBucketConfiguration={'LocationConstraint': sess.region_name})\n",
    "except Exception as e:\n",
    "    print(\"Looks like you already have a bucket of this name. That's good. Uploading the data files...\")\n",
    "\n",
    "# Return the URLs of the uploaded file, so they can be reviewed or used elsewhere\n",
    "s3url = S3Uploader.upload('data/train.csv', 's3://{}/{}/{}'.format(bucket, prefix,'train'))\n",
    "print(s3url)\n",
    "s3url = S3Uploader.upload('data/validation.csv', 's3://{}/{}/{}'.format(bucket, prefix,'validation'))\n",
    "print(s3url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create `s3_input`s that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugger\n",
    "\n",
    "With Amazon SageMaker Debugger, you can debug models during training. During training, Debugger periodically saves tensors, which specify the state of the model at that point in time. Debugger saves the tensors to Amazon S3 for analysis and visualization. This allows you to diagnose training issues with Studio.\n",
    "\n",
    "To enable automated detection of common issues during training, you can attach a list of rules to evaluate the training job against. Some rule configurations that apply to XGBoost include `AllZero`, `ClassImbalance`, `Confusion`, `LossNotDecreasing`, `Overfit`, `Overtraining`, `SimilarAcrossRuns`, `TensorVariance`, `UnchangedTensor`, `TreeDepth`.\n",
    "\n",
    "Let's use the `LossNotDecreasing` rule which is triggered if the loss doesn't decrease monotonically at any point during training, the `Overtraining` rule, and the `Overfit` rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_rules = [Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "               Rule.sagemaker(rule_configs.overtraining()),\n",
    "               Rule.sagemaker(rule_configs.overfit())\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "Amazon SageMaker Experiments allows us to keep track of model training; organize related models together; and log model configuration, parameters, and metrics so we can reproduce and iterate on previously trained models and compare models. We'll create a single experiment to keep track of the different trials to train the model that we'll try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.session.Session()\n",
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "customer_churn_experiment = Experiment.create(experiment_name=\"customer-churn-prediction-xgboost-{}\".format(create_date), \n",
    "                                              description=\"Using xgboost to predict customer churn\", \n",
    "                                              sagemaker_boto_client=boto3.client('sagemaker'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a trial for this run and associate it with the experiment that we just created so that we can use Studio to compare it with other trials. To train the model, we'll create an estimator and specify a few parameters, such as the type of training instances we'd like to use and how many and [hyperparameters](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst). Because we are running in framework mode, we also need to pass parameters, like the entry point script that can incorporate additional processing into your training jobs and the framework version, to the estimator.\n",
    "\n",
    "> We've made a couple of simple changes to enable the Debugger. We created a SessionHook which we pass as a callback function when creating a Booster. We passed a SaveConfig object that tells the hook to save the evaluation metrics, feature importances, and SHAP values at regular intervals. Debugger is highly configurable, so you can choose exactly what to save. For even more detail, see the [Developer Guide for XGBoost](https://github.com/awslabs/sagemaker-debugger/tree/master/docs/xgboost.md).\n",
    "<code>!pygmentize xgboost_customer_churn.py</code>\n",
    "\n",
    "After that, we call `fit` to start the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==**IMPORTANT**==\n",
    "\n",
    "The last line in the given [script](https://github.com/aws/amazon-sagemaker-examples/blob/master/aws_sagemaker_studio/getting_started/xgboost_customer_churn.py) must be modified, otherwise the notebook will have errors (See the [issue](https://github.com/aws/amazon-sagemaker-examples/issues/2031#issuecomment-787990970))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\"max_depth\":5,\n",
    "               \"subsample\":0.8,\n",
    "               \"num_round\":600,\n",
    "               \"eta\":0.2,\n",
    "               \"gamma\":4,\n",
    "               \"min_child_weight\":6,\n",
    "               \"silent\":0,\n",
    "               \"objective\":'binary:logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point_script = \"xgboost_customer_churn.py\"\n",
    "\n",
    "trial = Trial.create(trial_name=\"trial-{}-weight-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()), hyperparams['min_child_weight']), \n",
    "                     experiment_name=customer_churn_experiment.experiment_name,\n",
    "                     sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "framework_xgb = sagemaker.xgboost.XGBoost(entry_point=entry_point_script,\n",
    "                                          role=role,\n",
    "                                          framework_version=\"0.90-2\",\n",
    "                                          py_version=\"py3\",\n",
    "                                          hyperparameters=hyperparams,\n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.m5.xlarge',\n",
    "                                          output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                          sagemaker_session=sess,\n",
    "                                          rules=debug_rules\n",
    "                                          )\n",
    "\n",
    "framework_xgb.fit({'train': s3_input_train,\n",
    "                   'validation': s3_input_validation}, \n",
    "                  experiment_config={\n",
    "                      \"ExperimentName\": customer_churn_experiment.experiment_name, \n",
    "                      \"TrialName\": trial.trial_name,\n",
    "                      \"TrialComponentDisplayName\": \"Training\",\n",
    "                  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve a model, you typically try other hyperparameter values to see if they affect the final validation error. Let's change the `min_child_weight` value and start other training jobs with those values to see how they affect the validation error. For each `min_child_weight` value, we'll create a separate trial so that we can compare the results in Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_child_weights = [1, 2, 4, 8, 10]\n",
    "\n",
    "for weight in min_child_weights:\n",
    "    hyperparams[\"min_child_weight\"] = weight\n",
    "    trial = Trial.create(trial_name=\"trial-{}-weight-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()), weight), \n",
    "                         experiment_name=customer_churn_experiment.experiment_name,\n",
    "                         sagemaker_boto_client=boto3.client('sagemaker'))\n",
    "\n",
    "    t_xgb = sagemaker.xgboost.XGBoost(entry_point=entry_point_script,\n",
    "                                      role=role,\n",
    "                                      framework_version=\"0.90-2\",\n",
    "                                      py_version=\"py3\",\n",
    "                                      hyperparameters=hyperparams,\n",
    "                                      train_instance_count=1, \n",
    "                                      train_instance_type='ml.m5.xlarge',\n",
    "                                      output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                      sagemaker_session=sess,\n",
    "                                      rules=debug_rules\n",
    "                                     )\n",
    "\n",
    "    t_xgb.fit({'train': s3_input_train,\n",
    "               'validation': s3_input_validation},\n",
    "                wait=False,\n",
    "                experiment_config={\n",
    "                    \"ExperimentName\": customer_churn_experiment.experiment_name, \n",
    "                    \"TrialName\": trial.trial_name,\n",
    "                    \"TrialComponentDisplayName\": \"Training\",\n",
    "                }\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training jobs succeed, you can view `Experiments and trails` in Studio via `Sagemaker Components and registries` in the left bar. To see the trials of a specific experiment, in the Experiments list, double-click it. To see several trials together, multi-select them and right-click to choose `Open in trial component list`. You can click the button next to `Deploy model` to view more properties in the table, e.g., `Metrics`, `Parameters`, `Input artifacts` and `Output artifacts`. To create charts, you multi-select the trials then click `Add chart`. Here is an example: for `Data type`, choose `Summary statistics`; for `Chart type`, choose `Scatter plot`; then choose the `min_child_wight` hyperparameter as the `X-axis`; for `Y-axis` metrics, choose either `validation:error_last` or `validation:error_avg`, and then color them by `trialComponentName`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor\n",
    "\n",
    "Now that we've trained the model, let's deploy it to a hosted endpoint. To monitor the model after it's hosted and serving requests, we will also add configurations to capture data that is being sent to the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"xgboost-churn-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "job_name = 'sagemaker-xgboost-2021-03-01-13-52-39-970'\n",
    "xgb = sagemaker.xgboost.XGBoost.attach(job_name)\n",
    "data_capture_prefix = '{}/datacapture'.format(prefix)\n",
    "\n",
    "xgb_predictor = xgb.deploy(initial_instance_count=1, \n",
    "                           instance_type='ml.m5.large',\n",
    "                           endpoint_name=endpoint_name,\n",
    "                           data_capture_config=DataCaptureConfig(enable_capture=True,\n",
    "                                                                 sampling_percentage=100,\n",
    "                                                                 destination_s3_uri='s3://{}/{}'.format(bucket, data_capture_prefix)\n",
    "                                                                )\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model by making an HTTP POST request. But first, we'll need to set up serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll loop over our test dataset and collect predictions by invoking the XGBoost endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sending test traffic to the endpoint {}. \\nPlease wait for a minute...\".format(endpoint_name))\n",
    "\n",
    "with open('data/test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = xgb_predictor.predict(data=payload)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we made some real-time predictions by sending data to our endpoint, we should have also captured that data for monitoring purposes. Let's list the data capture files stored in Amazon S3. Expect to see different files from different time periods organized by the hour in which the invocation occurred. The format of the S3 path is:\n",
    "\n",
    "`s3://{bucket}/{data_capture_prefix}/{endpoint_name}/AllTraffic/yyyy/mm/dd/hh/filename.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "current_endpoint_capture_prefix = '{}/{}'.format(data_capture_prefix, endpoint_name)\n",
    "for _ in range(12): # wait up to a minute to see captures in S3\n",
    "    capture_files = S3Downloader.list(\"s3://{}/{}\".format(bucket, current_endpoint_capture_prefix))\n",
    "    if capture_files:\n",
    "        break\n",
    "    sleep(5)\n",
    "\n",
    "print(\"Found Data Capture Files:\")\n",
    "print(capture_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the data captured is stored in a SageMaker specific json-line formatted file. Next, let's take a quick peek at the contents of a single line in a pretty formatted json so that we can observe the format a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_file = S3Downloader.read_file(capture_files[-1])\n",
    "\n",
    "print(\"=====Single Data Capture====\")\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2)[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each inference request is captured in one line in the jsonl file. The line contains both the input and output merged together. In our example, we provided the ContentType as `text/csv` which is reflected in the `observedContentType` value. Also, we expose the enconding that we used to encode the input and output payloads in the capture format with the `encoding` value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Amazon SageMaker Model Monitor, we need to:\n",
    "1. Create a baseline to compare against real-time traffic. \n",
    "1. When the baseline is ready, set up a schedule to continously evaluate and compare against the baseline.\n",
    "1. Send synthetic traffic to trigger alarms.\n",
    "\n",
    "It takes one hour or more to complete this section because the shortest monitoring polling time is one hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Suggest baseline constraints with the training dataset\n",
    "\n",
    "The training dataset that you used to train the model is usually a good baseline dataset. Note that the training dataset data schema and the inference dataset schema should match exactly (for example, the number and type of the features).\n",
    "\n",
    "From our training dataset, let's ask Amazon SageMaker to suggest a set of baseline `constraints` and generate descriptive `statistics` to explore the data. For this example, let's upload the training dataset that we used to train the model. We'll use the dataset file with column headers so that we have descriptive feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_prefix = prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(bucket, baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(bucket, baseline_results_prefix)\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))\n",
    "baseline_data_path = S3Uploader.upload(\"data/training-dataset-with-header.csv\", baseline_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a baselining job with the training dataset\n",
    "\n",
    "Now that we have the training data ready in Amazon S3, let's start a job to `suggest` constraints. To generate the constraints, the convenient helper starts a `ProcessingJob` using a ProcessingJob container provided by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_default_monitor = DefaultModelMonitor(role=role,\n",
    "                                         instance_count=1,\n",
    "                                         instance_type='ml.m5.xlarge',\n",
    "                                         volume_size_in_gb=20,\n",
    "                                         max_runtime_in_seconds=3600,\n",
    "                                        )\n",
    "\n",
    "baseline_job = my_default_monitor.suggest_baseline(baseline_dataset=baseline_data_path,\n",
    "                                                   dataset_format=DatasetFormat.csv(header=True),\n",
    "                                                   output_s3_uri=baseline_results_uri,\n",
    "                                                   wait=True\n",
    "                                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job succeeds, we can explore the `baseline_results_uri` to see what files are stored there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Found Files:\")\n",
    "S3Downloader.list(\"s3://{}/{}\".format(bucket, baseline_results_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a `constraints.json` file that has information about suggested constraints. We also have a `statistics.json` file which contains statistical information about the data in the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyze subsequent captures for data quality issues\n",
    "\n",
    "Now that we have processed the baseline dataset to get baseline statistics and constraints, let's  monitor and analyze the data that is being sent to the endpoint with monitoring schedules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a monitoring schedule for the previously created endpoint. The schedule specifies the cadence at which we run a new processing job to compare recent data captures to the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, copy over some test scripts to the S3 bucket so that they can be used for pre and post processing\n",
    "code_prefix = '{}/code'.format(prefix)\n",
    "preprocessor_script = S3Uploader.upload('preprocessor.py', 's3://{}/{}'.format(bucket, code_prefix))\n",
    "postprocessor_script = S3Uploader.upload('postprocessor.py', 's3://{}/{}'.format(bucket, code_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create a model monitoring schedule for the endpoint created before and also the baseline resources (constraints and statistics) which were generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "reports_path = 's3://{}/{}'.format(bucket, reports_prefix)\n",
    "\n",
    "mon_schedule_name = 'demo-xgboost-customer-churn-model-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "my_default_monitor.create_monitoring_schedule(monitor_schedule_name=mon_schedule_name,\n",
    "                                              endpoint_input=xgb_predictor.endpoint,\n",
    "                                              #record_preprocessor_script=preprocessor_script,\n",
    "                                              post_analytics_processor_script=postprocessor_script,\n",
    "                                              output_s3_uri=reports_path,\n",
    "                                              statistics=my_default_monitor.baseline_statistics(),\n",
    "                                              constraints=my_default_monitor.suggested_constraints(),\n",
    "                                              schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "                                              enable_cloudwatch_metrics=True,\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start generating some artificial traffic\n",
    "\n",
    "The following block starts a thread to send some traffic to the created endpoint. This allows us to continue to send traffic to the endpoint so that we'll continually capture data for analysis. If there is no traffic, the monitoring jobs start to fail later.\n",
    "\n",
    "To terminate this thread, you need to stop the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# (just repeating code from above for convenience/ able to run this section independently)\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    with open(file_name, 'r') as f:\n",
    "        for row in f:\n",
    "            payload = row.rstrip('\\n')\n",
    "            response = runtime_client.invoke_endpoint(EndpointName=ep_name,\n",
    "                                                      ContentType='text/csv',\n",
    "                                                      Body=payload)\n",
    "            response['Body'].read()\n",
    "            sleep(1)\n",
    "            \n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        invoke_endpoint(endpoint_name, 'data/test-dataset-input-cols.csv', runtime_client)\n",
    "        \n",
    "thread = Thread(target=invoke_endpoint_forever)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you've set up the schedule, jobs start at specified intervals. Let's list the latest five executions. If you run this code after creating the hourly schedule, you might find the executions empty. You might have to wait until you cross the hour boundary (in UTC) to see executions start. The following code includes the logic for waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = my_default_monitor.list_executions()\n",
    "if len(mon_executions) == 0:\n",
    "    print(\"We created a hourly schedule above and it will kick off executions ON the hour.\\nWe will have to wait till we hit the hour...\")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the latest execution and list the generated reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = mon_executions[-1]\n",
    "latest_execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Latest execution result: {}\".format(latest_execution.describe()['ExitMessage']))\n",
    "report_uri = latest_execution.output.destination\n",
    "\n",
    "print(\"Found Report Files:\")\n",
    "S3Downloader.list(report_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any violations compared to the baseline, they will be generated here. Let's list the violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "constraints_df = pd.io.json.json_normalize(violations.body_dict[\"violations\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plug in the processing job arn for a single execution of the monitoring into [this notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker_model_monitor/visualization/SageMaker-Model-Monitor-Visualize.ipynb) to see more detailed visualizations of the violations and distribution statistics of the data captue that was processed in that execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution.describe()['ProcessingJobArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Clean-up\n",
    "\n",
    "If you're done with this example, run the following cell.  This removes the hosted endpoint that you created and prevents you from being charged for any instances that might continue running. It also cleans up all artifacts related to the experiment. \n",
    "\n",
    "Delete S3 bucket\n",
    "\n",
    "Open the Amazon S3 [console](https://console.aws.amazon.com/s3/), and then delete the bucket that you created for storing model artifacts and datasets.\n",
    "\n",
    "Delete logs\n",
    "\n",
    "Open the Amazon CloudWatch [console](https://console.aws.amazon.com/cloudwatch/), and then delete all of the log groups that have names starting with `/aws/sagemaker/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sess.delete_monitoring_schedule(mon_schedule_name)\n",
    "except:\n",
    "    pass\n",
    "while True:\n",
    "    try:\n",
    "        print(\"Waiting for schedule to be deleted\")\n",
    "        sess.describe_monitoring_schedule(mon_schedule_name)\n",
    "        sleep(15)\n",
    "    except:\n",
    "        print(\"Schedule deleted\")\n",
    "        break\n",
    "\n",
    "sess.delete_endpoint(xgb_predictor.endpoint)\n",
    "\n",
    "def cleanup(experiment):\n",
    "    '''Clean up everything in the given experiment object'''\n",
    "    for trial_summary in experiment.list_trials():\n",
    "        trial = Trial.load(trial_name=trial_summary.trial_name)\n",
    "        \n",
    "        for trial_comp_summary in trial.list_trial_components():\n",
    "            trial_step=TrialComponent.load(trial_component_name=trial_comp_summary.trial_component_name)\n",
    "            print('Starting to delete TrialComponent..' + trial_step.trial_component_name)\n",
    "            sm.disassociate_trial_component(TrialComponentName=trial_step.trial_component_name, TrialName=trial.trial_name)\n",
    "            trial_step.delete()\n",
    "            time.sleep(1)\n",
    "         \n",
    "        trial.delete()\n",
    "    \n",
    "    experiment.delete()\n",
    "\n",
    "cleanup(customer_churn_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can list the items to delete manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "\n",
    "# List all experiments in the account\n",
    "print(sm.list_experiments())\n",
    "# List monitoring schedules\n",
    "print(sm.list_monitoring_schedules())\n",
    "# List endpoints\n",
    "print(sm.list_endpoints())\n",
    "# List endpoint configs\n",
    "print(sm.list_endpoint_configs())\n",
    "# List models\n",
    "sm.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a monitoring schedule\n",
    "name = 'demo-xgboost-customer-churn-model-schedule-2021-03-01-15-54-57'\n",
    "sm.delete_monitoring_schedule(MonitoringScheduleName=name)\n",
    "\n",
    "# Delete an endpoint\n",
    "name = 'xgboost-churn-2021-03-01-13-57-30'\n",
    "sm.delete_endpoint(EndpointName=name)\n",
    "\n",
    "# Delete an endpoint config\n",
    "name = 'xgboost-churn-2021-03-01-13-57-30'\n",
    "sm.delete_endpoint_config(EndpointConfigName=name)\n",
    "\n",
    "# Force to delete the experiment and associated trials, trial components under the experiment.\n",
    "name = 'customer-churn-prediction-xgboost-2021-03-01-10-19-35'\n",
    "experiment = Experiment.load(experiment_name=name, sagemaker_boto_client=sm)\n",
    "experiment.delete_all(action='--force')\n",
    "\n",
    "# Delete a model\n",
    "name = 'sagemaker-xgboost-2021-03-01-13-52-39-970'\n",
    "sm.delete_model(ModelName=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Example3\"></a>\n",
    "# Example 3: bring own trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Example4\"></a>\n",
    "# Example 4: bring own container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-north-1:243637512696:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
