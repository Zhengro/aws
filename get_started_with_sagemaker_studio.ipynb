{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Different Combinations of SageMaker Components\n",
    "\n",
    "1. [Example 1: build, train, and deploy _**all**_ on SageMaker](#Example1)\n",
    "\n",
    "2. [Example 2: bring own training script](#Example2)\n",
    "\n",
    "3. [Example 3: bring own trained model](#Example3)\n",
    "\n",
    "4. [Example 4: bring own container](#Example4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Example1\"></a>\n",
    "# Example 1: build, train, and deploy _**all**_ on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "1. Introduction\n",
    "2. Prerequisites and Preprocessing\n",
    "    1. Data ingestion\n",
    "    2. Data inspection\n",
    "    3. Data conversion\n",
    "    4. Data uploading\n",
    "3. Training\n",
    "4. Hosting\n",
    "5. Validating\n",
    "6. (Optional) Clean-up\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Using Amazon SageMaker's Built-in Linear Learner to Predict Whether a Handwritten Digit is a 0.\n",
    "\n",
    "> Amazon SageMaker's Linear Learner algorithm extends upon typical linear models by training many models in parallel, in a computationally efficient manner. Each model has a different set of hyperparameters, and then the algorithm finds the set that optimizes a specific criteria. This can provide substantially more accurate models than typical linear algorithms at the same, or lower, cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Preprocessing\n",
    "\n",
    "Kernel in SageMaker Studio: `Data Science`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker.Session().default_bucket()  # “sagemaker-{region}-{aws-account-id}”\n",
    "prefix = 'demo-linear-mnist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion\n",
    "\n",
    "Next, we read the dataset into memory, for preprocessing prior to training. This processing could be done *in situ* by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pickle, gzip\n",
    "\n",
    "# Manually download mnist.pkl.gz from https://www.kaggle.com/pablotab/mnistpklgz\n",
    "# Upload mnist.pkl.gz to ./datasets\n",
    "\n",
    "# Load the dataset\n",
    "with gzip.open('./datasets/mnist.pkl.gz', 'rb') as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection\n",
    "\n",
    "Once the dataset is imported, it's typical as part of the machine learning process to inspect the data, understand the distributions, and determine what type(s) of preprocessing might be needed. You can perform those tasks right here in the notebook. As an example, let's go ahead and look at one of the digits that is part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (2, 10)\n",
    "\n",
    "\n",
    "def show_digit(img, caption='', subplot=None):\n",
    "    if subplot == None:\n",
    "        _, (subplot) = plt.subplots(1, 1)\n",
    "    imgr = img.reshape((28, 28))\n",
    "    subplot.axis('off')\n",
    "    subplot.imshow(imgr, cmap='gray')\n",
    "    plt.title(caption)\n",
    "\n",
    "show_digit(train_set[0][30], 'This is a {}'.format(train_set[1][30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data conversion\n",
    "\n",
    "Since algorithms have particular input and output requirements, converting the dataset is also part of the process that a data scientist goes through prior to initiating training. In this particular case, the Amazon SageMaker implementation of Linear Learner takes recordIO-wrapped protobuf, where the data we have today is a pickle-ized numpy array on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "vectors = np.array([t.tolist() for t in train_set[0]]).astype('float32')\n",
    "labels = np.where(np.array([t.tolist() for t in train_set[1]]) == 0, 1, 0).astype('float32')\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, vectors, labels)\n",
    "buf.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data uploading\n",
    "\n",
    "Now that we've created our recordIO-wrapped protobuf, we'll need to upload it to S3, so that Amazon SageMaker training can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "key = 'recordio-pb-data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3url_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print(s3url_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also setup an output S3 location for the model artifact that will be output as the result of training with the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3url_output = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print(s3url_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "First let's specify our algorithm image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "image = get_image_uri(boto3.Session().region_name, 'linear-learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll kick off the base estimator, making sure to pass in the necessary hyperparameters.  Notice:\n",
    "- `feature_dim` is set to 784, which is the number of pixels in each 28 x 28 image.\n",
    "- `predictor_type` is set to 'binary_classifier' since we are trying to predict whether the image is or is not a 0.\n",
    "- `mini_batch_size` is set to 200.  This value can be tuned for relatively minor improvements in fit and speed, but selecting a reasonable value relative to the dataset is appropriate in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "linear = sagemaker.estimator.Estimator(image,\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.m5.xlarge',\n",
    "                                       output_path=s3url_output,\n",
    "                                       sagemaker_session=sagemaker.Session())\n",
    "\n",
    "linear.set_hyperparameters(feature_dim=784,\n",
    "                           predictor_type='binary_classifier',\n",
    "                           mini_batch_size=200)\n",
    "\n",
    "linear.fit({'train': s3url_train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting\n",
    "\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint.  This will allow us to make predictions (or inference) from the model dyanamically. (_Note, Amazon SageMaker allows you the flexibility of importing models trained elsewhere, as well as the choice of not importing models if the target of model creation is AWS Lambda, AWS Greengrass, Amazon Redshift, Amazon Athena, or other deployment target._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Use an existing model trained by SageMaker\n",
    "\n",
    "job_name = 'linear-learner-2021-02-20-17-21-37-544'\n",
    "linear = sagemaker.estimator.Estimator.attach(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instance_types = [\"ml.r5d.12xlarge\", \"ml.r5.12xlarge\", \"ml.p2.xlarge\", \"ml.m5.4xlarge\", \"ml.m4.16xlarge\", \"ml.r5d.24xlarge\", \"ml.r5.24xlarge\", \"ml.p3.16xlarge\", \"ml.m5d.xlarge\", \"ml.m5.large\", \"ml.t2.xlarge\", \"ml.p2.16xlarge\", \"ml.m5d.12xlarge\", \"ml.inf1.2xlarge\", \"ml.m5d.24xlarge\", \"ml.c4.2xlarge\", \"ml.c5.2xlarge\", \"ml.c4.4xlarge\", \"ml.inf1.6xlarge\", \"ml.c5d.2xlarge\", \"ml.c5.4xlarge\", \"ml.g4dn.xlarge\", \"ml.g4dn.12xlarge\", \"ml.c5d.4xlarge\", \"ml.g4dn.2xlarge\", \"ml.c4.8xlarge\", \"ml.c4.large\", \"ml.c5d.xlarge\", \"ml.c5.large\", \"ml.g4dn.4xlarge\", \"ml.c5.9xlarge\", \"ml.g4dn.16xlarge\", \"ml.c5d.large\", \"ml.c5.xlarge\", \"ml.c5d.9xlarge\", \"ml.c4.xlarge\", \"ml.inf1.xlarge\", \"ml.g4dn.8xlarge\", \"ml.inf1.24xlarge\", \"ml.m5d.2xlarge\", \"ml.t2.2xlarge\", \"ml.c5d.18xlarge\", \"ml.m5d.4xlarge\", \"ml.t2.medium\", \"ml.c5.18xlarge\", \"ml.r5d.2xlarge\", \"ml.r5.2xlarge\", \"ml.p3.2xlarge\", \"ml.m5d.large\", \"ml.m5.xlarge\", \"ml.m4.10xlarge\", \"ml.t2.large\", \"ml.r5d.4xlarge\", \"ml.r5.4xlarge\", \"ml.m5.12xlarge\", \"ml.m4.xlarge\", \"ml.m5.24xlarge\", \"ml.m4.2xlarge\", \"ml.p2.8xlarge\", \"ml.m5.2xlarge\", \"ml.r5d.xlarge\", \"ml.r5d.large\", \"ml.r5.xlarge\", \"ml.r5.large\", \"ml.p3.8xlarge\", \"ml.m4.4xlarge\"]\n",
    "for i in range(len(all_instance_types)):\n",
    "    try:\n",
    "        linear_predictor = linear.deploy(initial_instance_count=1,\n",
    "                                         instance_type=all_instance_types[i])\n",
    "    except:\n",
    "        pass\n",
    "    else:\n",
    "        print('\\nUsing instance type: ', all_instance_types[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating\n",
    "\n",
    "Finally, we can now validate the model for use.  We can pass HTTP POST requests to the endpoint to get back predictions.  We'll specify how to serialize requests and deserialize responses that are specific to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "# linear_predictor.content_type = 'text/csv'\n",
    "linear_predictor.serializer = csv_serializer\n",
    "linear_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to get a prediction for a single record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = linear_predictor.predict(train_set[0][30:31])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see that for one record our endpoint returned some JSON which contains `predictions`, including the `score` and `predicted_label`.  In this case, `score` will be a continuous value between [0, 1] representing the probability we think the digit is a 0 or not.  `predicted_label` will take a value of either `0` or `1` where (somewhat counterintuitively) `1` denotes that we predict the image is a 0, while `0` denotes that we are predicting the image is not of a 0.\n",
    "\n",
    "Let's do a whole batch of images and evaluate our predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = []\n",
    "for array in np.array_split(test_set[0], 100):\n",
    "    result = linear_predictor.predict(array)\n",
    "    predictions += [r['predicted_label'] for r in result['predictions']]\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.crosstab(np.where(test_set[1] == 0, 1, 0), predictions, rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Clean-up\n",
    "\n",
    "To avoid incurring unnecessary charges, delete the endpoints and resources that you created while running this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(linear_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pprint import pprint\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "\n",
    "def main():\n",
    "    model_names = []\n",
    "    for key in paginate(client.list_models):\n",
    "        model_names.append(key['ModelName'])\n",
    "    delete_multiple_models(model_names)\n",
    "\n",
    "\n",
    "def delete_multiple_models(model_names):\n",
    "    for model_name in model_names:\n",
    "        print('Deleting model: {}'.format(model_name))\n",
    "        client.delete_model(ModelName=model_name)\n",
    "\n",
    "\n",
    "def paginate(method, **kwargs):\n",
    "    client = method.__self__\n",
    "    paginator = client.get_paginator(method.__name__)\n",
    "    for page in paginator.paginate(**kwargs).result_key_iters():\n",
    "        for result in page:\n",
    "            yield result\n",
    "\n",
    "            \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete S3 bucket\n",
    "\n",
    "Open the Amazon S3 [console](https://console.aws.amazon.com/s3/), and then delete the bucket that you created for storing model artifacts and the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete logs\n",
    "\n",
    "Open the Amazon CloudWatch [console](https://console.aws.amazon.com/cloudwatch/), and then delete all of the log groups that have names starting with `/aws/sagemaker/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Example2\"></a>\n",
    "# Example 2: bring own training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Example3\"></a>\n",
    "# Example 3: bring own trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Example4\"></a>\n",
    "# Example 4: bring own container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-north-1:243637512696:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
